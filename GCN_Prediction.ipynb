{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reactome data proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通路的节点连接\n",
    "data = pd.read_csv('./data/ReactomePathwaysRelation_new_download.txt',sep = '\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通路的特征信息（基因表示）\n",
    "import re\n",
    "def load_data_dict(filename):\n",
    "\n",
    "    data_dict_list = []\n",
    "    dict = {}\n",
    "    with open( filename) as gmt:\n",
    "        data_list = gmt.readlines()\n",
    "        \n",
    "        for row in data_list:\n",
    "            genes = row.split('\\t')\n",
    "            \n",
    "            genes = [ i.replace('\\n','') for i in genes]\n",
    "            dict[genes[1]] = genes[3:]\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "gene_data = load_data_dict('./data/ReactomePathways.gmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选出HSA的通路\n",
    "data = data[data[0].str.contains('HSA')]  #选出HSA 的通路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2029"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#只取存在特征的通路节点\n",
    "total_paths = list(set(gene_data.keys()))\n",
    "len(total_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除不在通路的节点\n",
    "data[3] = data[0].apply(lambda x : 1 if x in total_paths else 0 )\n",
    "data[4] = data[1].apply(lambda x : 1 if x in total_paths else 0 )\n",
    "data1 = data[data[3]==1]\n",
    "data2= data1[data1[4]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化index\n",
    "data2.reset_index(drop = True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造临接矩阵\n",
    "row , col = list(total_paths),list(total_paths)\n",
    "\n",
    "paths_matrix = np.zeros([len(row),len(col)])\n",
    "\n",
    "for  indexs in range(data2.shape[0]):\n",
    "      r = data2.loc[indexs][0]\n",
    "      c = data2.loc[indexs][1]\n",
    "      pos_r = row.index(r)\n",
    "      pos_c = col.index(c)\n",
    "      paths_matrix[pos_r,pos_c] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2028, 2028, 2028],\n",
       "       [ 136,  306,  544, ...,  548,  574, 1151]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(paths_matrix.nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "paths_matrix = sparse.csr_matrix(paths_matrix)  # array 转成 scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2028, 2028, 2028],\n",
       "       [ 136,  306,  544, ...,  548,  574, 1151]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(paths_matrix.nonzero())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造特征矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10690"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取图上节点的全部基因\n",
    "total_gene = []\n",
    "for index in total_paths:\n",
    "     total_gene = total_gene + gene_data[index]  \n",
    "    \n",
    "total_gene_set = list(set(total_gene))\n",
    "len(total_gene_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.zeros((len(total_paths), len(total_gene_set))) #  生成矩阵 【行 列】\n",
    "for p in total_paths:   \n",
    "    gs =  gene_data[p]\n",
    "    #  得到key 和 values  \n",
    "    g_inds = [total_gene_set.index(g) for g in gs]   #   获取基因在矩阵的位置\n",
    "    p_ind = total_paths.index(p)                      #   获取 通路在矩阵的位置\n",
    "    mat[p_ind,g_inds] = 1\n",
    "\n",
    "df = pd.DataFrame(mat, index=total_paths, columns=total_gene_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建图数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\GNN\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.data import Data\n",
    "#这里给出大家注释方便理解\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    #返回数据集源文件名\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2', ...]\n",
    "    \n",
    "    #返回process方法所需的保存文件名。你之后保存的数据集名字和列表里的一致\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "    \n",
    "    \n",
    "    #生成数据集\n",
    "    def process(self):\n",
    "        \n",
    "        edge_index = torch.tensor(np.array(paths_matrix.nonzero()), dtype=torch.long)   #边的关系\n",
    " \n",
    "        X = torch.tensor(df.values, dtype=torch.float)  #节点特征\n",
    "    \n",
    "        data = Data(x=X, edge_index=edge_index,)\n",
    "    \n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = [data]\n",
    " \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    " \n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    " \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "MyOwn =MyOwnDataset('ReactomeGCNData1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2029, 10690], edge_index=[2, 2003])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyOwn.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2028, 2028, 2028],\n",
       "        [ 136,  306,  544,  ...,  548,  574, 1151]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyOwn.data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(row[2],row[1294])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling, train_test_split_edges\n",
    "\n",
    "data = train_test_split_edges(MyOwn.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2029, 10690], val_pos_edge_index=[2, 50], test_pos_edge_index=[2, 100], train_pos_edge_index=[2, 1704], train_neg_adj_mask=[2029, 2029], val_neg_edge_index=[2, 50], test_neg_edge_index=[2, 100])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.num_features = data.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 256)\n",
    "        self.conv2 = GCNConv(256, 128)\n",
    "        self.conv3 = GCNConv(128, out_channels)\n",
    "        \n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, pos_edge_index, neg_edge_index):\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)  # [2,E]\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # *：element-wise乘法\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()  # @：矩阵乘法，自动执行适合的矩阵乘法函数\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "    def forward(self, x, pos_edge_index, neg_edge_index):\n",
    "        return decode(encode(x, pos_edge_index), pos_edge_index, neg_edge_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    num_links = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(num_links, dtype=torch.float, device=device)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train(data, model, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(  # 训练集负采样，每个epoch负采样样本可能不同\n",
    "        edge_index=data.train_pos_edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # link_logits = model(data.x, data.train_pos_edge_index, neg_edge_index)\n",
    "    z = model.encode(data.x, data.train_pos_edge_index)\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index)\n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(data.x.device)  # 训练集中正样本标签\n",
    "    loss = criterion(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def mytest(data,model):\n",
    "    model.eval()\n",
    "\n",
    "    z = model.encode(data.x, data.train_pos_edge_index)\n",
    "\n",
    "    results = []\n",
    "    for prefix in ['val', 'test']:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
    "        link_probs = link_logits.sigmoid()#计算链路存在的概率\n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "        results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(data.num_features, 64).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "best_val_auc = test_auc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6160, Val: 0.7748, Test: 0.8126\n",
      "Epoch: 002, Loss: 2.9047, Val: 0.5804, Test: 0.8126\n",
      "Epoch: 003, Loss: 26.5050, Val: 0.8206, Test: 0.8399\n",
      "Epoch: 004, Loss: 1.9961, Val: 0.8110, Test: 0.8399\n",
      "Epoch: 005, Loss: 1.4101, Val: 0.6780, Test: 0.8399\n",
      "Epoch: 006, Loss: 1.8808, Val: 0.6756, Test: 0.8399\n",
      "Epoch: 007, Loss: 1.3100, Val: 0.6960, Test: 0.8399\n",
      "Epoch: 008, Loss: 0.8006, Val: 0.7360, Test: 0.8399\n",
      "Epoch: 009, Loss: 0.6899, Val: 0.7768, Test: 0.8399\n",
      "Epoch: 010, Loss: 0.5457, Val: 0.7932, Test: 0.8399\n",
      "Epoch: 011, Loss: 0.5435, Val: 0.7972, Test: 0.8399\n",
      "Epoch: 012, Loss: 0.5374, Val: 0.8096, Test: 0.8399\n",
      "Epoch: 013, Loss: 0.5410, Val: 0.8044, Test: 0.8399\n",
      "Epoch: 014, Loss: 0.5179, Val: 0.8012, Test: 0.8399\n",
      "Epoch: 015, Loss: 0.5142, Val: 0.7932, Test: 0.8399\n",
      "Epoch: 016, Loss: 0.5023, Val: 0.7996, Test: 0.8399\n",
      "Epoch: 017, Loss: 0.4919, Val: 0.8092, Test: 0.8399\n",
      "Epoch: 018, Loss: 0.4823, Val: 0.8112, Test: 0.8399\n",
      "Epoch: 019, Loss: 0.4876, Val: 0.8152, Test: 0.8399\n",
      "Epoch: 020, Loss: 0.4787, Val: 0.8212, Test: 0.8646\n",
      "Epoch: 021, Loss: 0.4629, Val: 0.8228, Test: 0.8685\n",
      "Epoch: 022, Loss: 0.4550, Val: 0.8252, Test: 0.8735\n",
      "Epoch: 023, Loss: 0.4644, Val: 0.8308, Test: 0.8764\n",
      "Epoch: 024, Loss: 0.4725, Val: 0.8304, Test: 0.8764\n",
      "Epoch: 025, Loss: 0.4496, Val: 0.8280, Test: 0.8764\n",
      "Epoch: 026, Loss: 0.4370, Val: 0.8284, Test: 0.8764\n",
      "Epoch: 027, Loss: 0.4386, Val: 0.8316, Test: 0.8821\n",
      "Epoch: 028, Loss: 0.4272, Val: 0.8308, Test: 0.8821\n",
      "Epoch: 029, Loss: 0.4270, Val: 0.8356, Test: 0.8866\n",
      "Epoch: 030, Loss: 0.4277, Val: 0.8412, Test: 0.8857\n",
      "Epoch: 031, Loss: 0.4274, Val: 0.8452, Test: 0.8865\n",
      "Epoch: 032, Loss: 0.4138, Val: 0.8516, Test: 0.8891\n",
      "Epoch: 033, Loss: 0.4102, Val: 0.8540, Test: 0.8894\n",
      "Epoch: 034, Loss: 0.4031, Val: 0.8564, Test: 0.8901\n",
      "Epoch: 035, Loss: 0.4127, Val: 0.8576, Test: 0.8890\n",
      "Epoch: 036, Loss: 0.4031, Val: 0.8600, Test: 0.8918\n",
      "Epoch: 037, Loss: 0.3992, Val: 0.8596, Test: 0.8918\n",
      "Epoch: 038, Loss: 0.4047, Val: 0.8616, Test: 0.8930\n",
      "Epoch: 039, Loss: 0.4081, Val: 0.8600, Test: 0.8930\n",
      "Epoch: 040, Loss: 0.3939, Val: 0.8592, Test: 0.8930\n",
      "Epoch: 041, Loss: 0.3925, Val: 0.8616, Test: 0.8931\n",
      "Epoch: 042, Loss: 0.3830, Val: 0.8636, Test: 0.8946\n",
      "Epoch: 043, Loss: 0.4036, Val: 0.8636, Test: 0.8949\n",
      "Epoch: 044, Loss: 0.3997, Val: 0.8652, Test: 0.8952\n",
      "Epoch: 045, Loss: 0.3928, Val: 0.8700, Test: 0.8954\n",
      "Epoch: 046, Loss: 0.3917, Val: 0.8748, Test: 0.8972\n",
      "Epoch: 047, Loss: 0.3906, Val: 0.8772, Test: 0.8995\n",
      "Epoch: 048, Loss: 0.3861, Val: 0.8792, Test: 0.9009\n",
      "Epoch: 049, Loss: 0.3860, Val: 0.8792, Test: 0.9009\n",
      "Epoch: 050, Loss: 0.3880, Val: 0.8800, Test: 0.9034\n",
      "Epoch: 051, Loss: 0.3818, Val: 0.8832, Test: 0.9045\n",
      "Epoch: 052, Loss: 0.3834, Val: 0.8836, Test: 0.9024\n",
      "Epoch: 053, Loss: 0.3823, Val: 0.8808, Test: 0.9024\n",
      "Epoch: 054, Loss: 0.3843, Val: 0.8796, Test: 0.9024\n",
      "Epoch: 055, Loss: 0.3893, Val: 0.8780, Test: 0.9024\n",
      "Epoch: 056, Loss: 0.3837, Val: 0.8712, Test: 0.9024\n",
      "Epoch: 057, Loss: 0.3822, Val: 0.8672, Test: 0.9024\n",
      "Epoch: 058, Loss: 0.3826, Val: 0.8568, Test: 0.9024\n",
      "Epoch: 059, Loss: 0.3786, Val: 0.8504, Test: 0.9024\n",
      "Epoch: 060, Loss: 0.3909, Val: 0.8484, Test: 0.9024\n",
      "Epoch: 061, Loss: 0.3777, Val: 0.8488, Test: 0.9024\n",
      "Epoch: 062, Loss: 0.3860, Val: 0.8516, Test: 0.9024\n",
      "Epoch: 063, Loss: 0.3784, Val: 0.8572, Test: 0.9024\n",
      "Epoch: 064, Loss: 0.3785, Val: 0.8612, Test: 0.9024\n",
      "Epoch: 065, Loss: 0.3836, Val: 0.8588, Test: 0.9024\n",
      "Epoch: 066, Loss: 0.3767, Val: 0.8588, Test: 0.9024\n",
      "Epoch: 067, Loss: 0.3864, Val: 0.8588, Test: 0.9024\n",
      "Epoch: 068, Loss: 0.3737, Val: 0.8604, Test: 0.9024\n",
      "Epoch: 069, Loss: 0.3773, Val: 0.8764, Test: 0.9024\n",
      "Epoch: 070, Loss: 0.3789, Val: 0.8792, Test: 0.9024\n",
      "Epoch: 071, Loss: 0.3722, Val: 0.8796, Test: 0.9024\n",
      "Epoch: 072, Loss: 0.3785, Val: 0.8816, Test: 0.9024\n",
      "Epoch: 073, Loss: 0.3747, Val: 0.8824, Test: 0.9024\n",
      "Epoch: 074, Loss: 0.3783, Val: 0.8856, Test: 0.9028\n",
      "Epoch: 075, Loss: 0.3782, Val: 0.8864, Test: 0.9024\n",
      "Epoch: 076, Loss: 0.3724, Val: 0.8868, Test: 0.9023\n",
      "Epoch: 077, Loss: 0.3757, Val: 0.8856, Test: 0.9023\n",
      "Epoch: 078, Loss: 0.3776, Val: 0.8856, Test: 0.9023\n",
      "Epoch: 079, Loss: 0.3731, Val: 0.8860, Test: 0.9023\n",
      "Epoch: 080, Loss: 0.3750, Val: 0.8864, Test: 0.9023\n",
      "Epoch: 081, Loss: 0.3755, Val: 0.8856, Test: 0.9023\n",
      "Epoch: 082, Loss: 0.3742, Val: 0.8880, Test: 0.9078\n",
      "Epoch: 083, Loss: 0.3769, Val: 0.8884, Test: 0.9088\n",
      "Epoch: 084, Loss: 0.3776, Val: 0.8892, Test: 0.9127\n",
      "Epoch: 085, Loss: 0.3718, Val: 0.8924, Test: 0.9158\n",
      "Epoch: 086, Loss: 0.3780, Val: 0.8928, Test: 0.9168\n",
      "Epoch: 087, Loss: 0.3796, Val: 0.8904, Test: 0.9168\n",
      "Epoch: 088, Loss: 0.3817, Val: 0.8904, Test: 0.9168\n",
      "Epoch: 089, Loss: 0.3759, Val: 0.8908, Test: 0.9168\n",
      "Epoch: 090, Loss: 0.3704, Val: 0.8904, Test: 0.9168\n",
      "Epoch: 091, Loss: 0.3751, Val: 0.8896, Test: 0.9168\n",
      "Epoch: 092, Loss: 0.3710, Val: 0.8892, Test: 0.9168\n",
      "Epoch: 093, Loss: 0.3717, Val: 0.8876, Test: 0.9168\n",
      "Epoch: 094, Loss: 0.3735, Val: 0.8888, Test: 0.9168\n",
      "Epoch: 095, Loss: 0.3669, Val: 0.8880, Test: 0.9168\n",
      "Epoch: 096, Loss: 0.3640, Val: 0.8892, Test: 0.9168\n",
      "Epoch: 097, Loss: 0.3770, Val: 0.8876, Test: 0.9168\n",
      "Epoch: 098, Loss: 0.3708, Val: 0.8852, Test: 0.9168\n",
      "Epoch: 099, Loss: 0.3743, Val: 0.8848, Test: 0.9168\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,100):\n",
    "    loss=train(data,model,optimizer,criterion)\n",
    "    val_auc,tmp_test_auc=mytest(data,model)\n",
    "    if val_auc>best_val_auc:\n",
    "        best_val_auc=val_auc\n",
    "        test_auc=tmp_test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, Test: {test_auc:.4f}')\n",
    "#预测\n",
    "# z=model.encode(data.x,data.train_pos_edge_index)\n",
    "# final_edge_index=model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model.encode(data.x,data.train_pos_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0914,  0.2032,  0.7769,  ...,  0.0441, -0.4794, -0.3830],\n",
       "        [-0.0371,  0.0172,  0.1125,  ...,  0.0162,  0.0194,  0.2207],\n",
       "        [ 0.0092,  0.0266, -0.0223,  ...,  0.0091,  0.0148,  0.0192],\n",
       "        ...,\n",
       "        [-0.0439, -0.2785,  0.1426,  ...,  0.0477, -0.2052, -0.0135],\n",
       "        [ 0.0959, -0.0035, -0.3102,  ...,  0.2329, -0.2712,  0.1266],\n",
       "        [-0.0116, -0.0141, -0.0135,  ...,  0.1289,  0.3460,  0.1629]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = pd.DataFrame(z.detach().numpy(),index = total_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R-HSA-8953854</th>\n",
       "      <td>0.091381</td>\n",
       "      <td>0.203161</td>\n",
       "      <td>0.776924</td>\n",
       "      <td>-0.558911</td>\n",
       "      <td>0.284520</td>\n",
       "      <td>-0.140107</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.945102</td>\n",
       "      <td>0.168385</td>\n",
       "      <td>-0.723967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749231</td>\n",
       "      <td>0.265940</td>\n",
       "      <td>0.389738</td>\n",
       "      <td>-0.124046</td>\n",
       "      <td>-0.586530</td>\n",
       "      <td>0.507120</td>\n",
       "      <td>-0.437051</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>-0.479429</td>\n",
       "      <td>-0.383033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-211733</th>\n",
       "      <td>-0.037075</td>\n",
       "      <td>0.017154</td>\n",
       "      <td>0.112517</td>\n",
       "      <td>-0.254793</td>\n",
       "      <td>-0.153845</td>\n",
       "      <td>-0.117749</td>\n",
       "      <td>-0.028945</td>\n",
       "      <td>-0.074176</td>\n",
       "      <td>0.035665</td>\n",
       "      <td>0.101395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235917</td>\n",
       "      <td>-0.088024</td>\n",
       "      <td>-0.127737</td>\n",
       "      <td>0.056212</td>\n",
       "      <td>-0.013957</td>\n",
       "      <td>-0.029609</td>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.019418</td>\n",
       "      <td>0.220693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-173599</th>\n",
       "      <td>0.009176</td>\n",
       "      <td>0.026553</td>\n",
       "      <td>-0.022261</td>\n",
       "      <td>-0.025813</td>\n",
       "      <td>-0.035659</td>\n",
       "      <td>-0.015939</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>-0.016199</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.013102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010685</td>\n",
       "      <td>-0.037684</td>\n",
       "      <td>-0.025810</td>\n",
       "      <td>0.033827</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>-0.044949</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.019203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-3560783</th>\n",
       "      <td>-0.255390</td>\n",
       "      <td>0.253245</td>\n",
       "      <td>-0.074782</td>\n",
       "      <td>0.317875</td>\n",
       "      <td>-0.370775</td>\n",
       "      <td>0.297177</td>\n",
       "      <td>-0.004499</td>\n",
       "      <td>-0.212764</td>\n",
       "      <td>0.356232</td>\n",
       "      <td>-0.118090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264449</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>-0.048571</td>\n",
       "      <td>0.248857</td>\n",
       "      <td>0.169930</td>\n",
       "      <td>0.108922</td>\n",
       "      <td>0.145072</td>\n",
       "      <td>-0.323338</td>\n",
       "      <td>0.266036</td>\n",
       "      <td>0.006522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-140877</th>\n",
       "      <td>-0.215039</td>\n",
       "      <td>-0.202694</td>\n",
       "      <td>0.122047</td>\n",
       "      <td>-0.057728</td>\n",
       "      <td>-0.232950</td>\n",
       "      <td>-0.292914</td>\n",
       "      <td>-0.105633</td>\n",
       "      <td>-0.233615</td>\n",
       "      <td>-0.044733</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103033</td>\n",
       "      <td>-0.184272</td>\n",
       "      <td>0.109887</td>\n",
       "      <td>-0.487025</td>\n",
       "      <td>0.103150</td>\n",
       "      <td>0.227254</td>\n",
       "      <td>-0.299837</td>\n",
       "      <td>0.447151</td>\n",
       "      <td>-0.373352</td>\n",
       "      <td>-0.053435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-349425</th>\n",
       "      <td>-0.034888</td>\n",
       "      <td>0.031946</td>\n",
       "      <td>0.119353</td>\n",
       "      <td>-0.290362</td>\n",
       "      <td>-0.159712</td>\n",
       "      <td>-0.115350</td>\n",
       "      <td>-0.034734</td>\n",
       "      <td>-0.071284</td>\n",
       "      <td>0.028227</td>\n",
       "      <td>0.118884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271280</td>\n",
       "      <td>-0.087017</td>\n",
       "      <td>-0.125474</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>-0.021660</td>\n",
       "      <td>-0.025991</td>\n",
       "      <td>0.226277</td>\n",
       "      <td>-0.001100</td>\n",
       "      <td>0.019383</td>\n",
       "      <td>0.255802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-8934593</th>\n",
       "      <td>0.043711</td>\n",
       "      <td>0.018287</td>\n",
       "      <td>0.020465</td>\n",
       "      <td>-0.002057</td>\n",
       "      <td>-0.017925</td>\n",
       "      <td>-0.033605</td>\n",
       "      <td>0.047126</td>\n",
       "      <td>-0.057995</td>\n",
       "      <td>-0.027391</td>\n",
       "      <td>0.009594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>-0.023501</td>\n",
       "      <td>-0.009044</td>\n",
       "      <td>0.010507</td>\n",
       "      <td>0.077307</td>\n",
       "      <td>-0.003307</td>\n",
       "      <td>-0.040056</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.038609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-6811434</th>\n",
       "      <td>-0.043857</td>\n",
       "      <td>-0.278549</td>\n",
       "      <td>0.142622</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>-0.464538</td>\n",
       "      <td>-0.278462</td>\n",
       "      <td>-0.003130</td>\n",
       "      <td>-0.073984</td>\n",
       "      <td>0.204335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457489</td>\n",
       "      <td>0.019473</td>\n",
       "      <td>-0.303331</td>\n",
       "      <td>0.211101</td>\n",
       "      <td>0.268194</td>\n",
       "      <td>-0.286352</td>\n",
       "      <td>-0.081349</td>\n",
       "      <td>0.047699</td>\n",
       "      <td>-0.205166</td>\n",
       "      <td>-0.013462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-166665</th>\n",
       "      <td>0.095913</td>\n",
       "      <td>-0.003483</td>\n",
       "      <td>-0.310165</td>\n",
       "      <td>0.127688</td>\n",
       "      <td>0.053658</td>\n",
       "      <td>-0.117361</td>\n",
       "      <td>-0.044907</td>\n",
       "      <td>-0.101881</td>\n",
       "      <td>-0.178735</td>\n",
       "      <td>0.213908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208097</td>\n",
       "      <td>-0.335245</td>\n",
       "      <td>-0.186561</td>\n",
       "      <td>-0.331478</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>0.041079</td>\n",
       "      <td>0.248687</td>\n",
       "      <td>0.232874</td>\n",
       "      <td>-0.271221</td>\n",
       "      <td>0.126650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-HSA-3229121</th>\n",
       "      <td>-0.011552</td>\n",
       "      <td>-0.014135</td>\n",
       "      <td>-0.013471</td>\n",
       "      <td>0.127006</td>\n",
       "      <td>0.104928</td>\n",
       "      <td>-0.137374</td>\n",
       "      <td>-0.188447</td>\n",
       "      <td>0.031078</td>\n",
       "      <td>-0.095989</td>\n",
       "      <td>0.147131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331252</td>\n",
       "      <td>0.088925</td>\n",
       "      <td>0.142667</td>\n",
       "      <td>-0.292858</td>\n",
       "      <td>0.155131</td>\n",
       "      <td>0.045044</td>\n",
       "      <td>-0.164938</td>\n",
       "      <td>0.128892</td>\n",
       "      <td>0.345960</td>\n",
       "      <td>0.162947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2029 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5    \\\n",
       "R-HSA-8953854  0.091381  0.203161  0.776924 -0.558911  0.284520 -0.140107   \n",
       "R-HSA-211733  -0.037075  0.017154  0.112517 -0.254793 -0.153845 -0.117749   \n",
       "R-HSA-173599   0.009176  0.026553 -0.022261 -0.025813 -0.035659 -0.015939   \n",
       "R-HSA-3560783 -0.255390  0.253245 -0.074782  0.317875 -0.370775  0.297177   \n",
       "R-HSA-140877  -0.215039 -0.202694  0.122047 -0.057728 -0.232950 -0.292914   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "R-HSA-349425  -0.034888  0.031946  0.119353 -0.290362 -0.159712 -0.115350   \n",
       "R-HSA-8934593  0.043711  0.018287  0.020465 -0.002057 -0.017925 -0.033605   \n",
       "R-HSA-6811434 -0.043857 -0.278549  0.142622 -0.003279  0.019841 -0.464538   \n",
       "R-HSA-166665   0.095913 -0.003483 -0.310165  0.127688  0.053658 -0.117361   \n",
       "R-HSA-3229121 -0.011552 -0.014135 -0.013471  0.127006  0.104928 -0.137374   \n",
       "\n",
       "                    6         7         8         9    ...       118  \\\n",
       "R-HSA-8953854 -0.000049  0.945102  0.168385 -0.723967  ...  0.749231   \n",
       "R-HSA-211733  -0.028945 -0.074176  0.035665  0.101395  ...  0.235917   \n",
       "R-HSA-173599   0.005500 -0.016199  0.005756  0.013102  ... -0.010685   \n",
       "R-HSA-3560783 -0.004499 -0.212764  0.356232 -0.118090  ... -0.264449   \n",
       "R-HSA-140877  -0.105633 -0.233615 -0.044733 -0.353424  ... -0.103033   \n",
       "...                 ...       ...       ...       ...  ...       ...   \n",
       "R-HSA-349425  -0.034734 -0.071284  0.028227  0.118884  ...  0.271280   \n",
       "R-HSA-8934593  0.047126 -0.057995 -0.027391  0.009594  ...  0.022240   \n",
       "R-HSA-6811434 -0.278462 -0.003130 -0.073984  0.204335  ... -0.457489   \n",
       "R-HSA-166665  -0.044907 -0.101881 -0.178735  0.213908  ... -0.208097   \n",
       "R-HSA-3229121 -0.188447  0.031078 -0.095989  0.147131  ... -0.331252   \n",
       "\n",
       "                    119       120       121       122       123       124  \\\n",
       "R-HSA-8953854  0.265940  0.389738 -0.124046 -0.586530  0.507120 -0.437051   \n",
       "R-HSA-211733  -0.088024 -0.127737  0.056212 -0.013957 -0.029609  0.196879   \n",
       "R-HSA-173599  -0.037684 -0.025810  0.033827  0.002167  0.009618 -0.044949   \n",
       "R-HSA-3560783  0.275320 -0.048571  0.248857  0.169930  0.108922  0.145072   \n",
       "R-HSA-140877  -0.184272  0.109887 -0.487025  0.103150  0.227254 -0.299837   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "R-HSA-349425  -0.087017 -0.125474  0.056010 -0.021660 -0.025991  0.226277   \n",
       "R-HSA-8934593 -0.023501 -0.009044  0.010507  0.077307 -0.003307 -0.040056   \n",
       "R-HSA-6811434  0.019473 -0.303331  0.211101  0.268194 -0.286352 -0.081349   \n",
       "R-HSA-166665  -0.335245 -0.186561 -0.331478  0.203362  0.041079  0.248687   \n",
       "R-HSA-3229121  0.088925  0.142667 -0.292858  0.155131  0.045044 -0.164938   \n",
       "\n",
       "                    125       126       127  \n",
       "R-HSA-8953854  0.044124 -0.479429 -0.383033  \n",
       "R-HSA-211733   0.016238  0.019418  0.220693  \n",
       "R-HSA-173599   0.009072  0.014811  0.019203  \n",
       "R-HSA-3560783 -0.323338  0.266036  0.006522  \n",
       "R-HSA-140877   0.447151 -0.373352 -0.053435  \n",
       "...                 ...       ...       ...  \n",
       "R-HSA-349425  -0.001100  0.019383  0.255802  \n",
       "R-HSA-8934593  0.002286  0.004713  0.038609  \n",
       "R-HSA-6811434  0.047699 -0.205166 -0.013462  \n",
       "R-HSA-166665   0.232874 -0.271221  0.126650  \n",
       "R-HSA-3229121  0.128892  0.345960  0.162947  \n",
       "\n",
       "[2029 rows x 128 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path.to_csv('./data/Pathways_Feature.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790995438476585"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z)) \n",
    "sigmoid(z[2].dot(z[1969]).detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
